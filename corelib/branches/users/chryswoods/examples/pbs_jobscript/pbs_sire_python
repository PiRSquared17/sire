#!/bin/env python

import os
import sys
import string
import re
import time
from socket import gethostname

if len(sys.argv) < 2:
    print "USAGE: pbs_sire_python /path/to/script_to_run.py (n_times_to_run)"
    raise SystemExit

if len(sys.argv) > 2:
    nruns = int(sys.argv[2])

else:
    nruns = 1

nruns = max(1, nruns)

runcmd = sys.argv[1]

path = string.join(runcmd.split("/")[0:-1], "/")
cmd = runcmd.split("/")[-1]

logfile = re.sub("\.py$", ".log", cmd)

#pipe stdout and stderr to a logfile so that they autoflush
sys.stdout = open("%s/pbs_sire_python.log" % path, 'a', 0)
sys.stderr = open("%s/pbs_sire_python.err" % path, 'a', 0)

print "Running %s in directory %s, writing output to %s" % (cmd,path,logfile)

if nruns > 1:
    print "Running this script %d times" % (nruns)

os.chdir(path)

if not os.path.exists(cmd):
    print "Cannot find the command file %s in directory %s!" % (cmd, path)
    sys.exit(-1)

# get the directory containing the mpi executables
# mpidir = os.getenv("MPIDIR")
#
#if mpidir is None:
#    print "You must set the MPIDIR environmental variable to point to the directory " + \
#          "containing mpirun and mpdboot"
#    sys.exit(-1)

# get the path to the temporary directory
tmpdir = os.getenv("TMPDIR")

if tmpdir is None:
    tmpdir = "/tmp"
    os.environ["TMPDIR"] = tmpdir

pbs_nodefile = os.getenv("PBS_NODEFILE")

if pbs_nodefile is None:
    print "There is no PBS_NODEFILE environmental variable. Is this job being run under PBS?"
    FILE = open("tmp_nodefile", "w")
    print >>FILE,gethostname()
    FILE.close()

    pbs_nodefile = "tmp_nodefile"

pbs_jobid = os.getenv("PBS_JOBID")
pbs_tmpdir = None

if pbs_jobid is None:
    print "There is no PBS job ID number - this means that this job can't be resubmitted!"
else:
    # create a job-specific TMPDIR (so we can clean up files)
    pbs_tmpdir = "%s/%s_jobid_%s" % (tmpdir, os.getenv("USER"), pbs_jobid)
    os.system("create_pbs_tmpdir %s" % pbs_tmpdir)
    os.environ["TMPDIR"] = pbs_tmpdir

#Set the environment variable "ONE_JOB_PER_NODE" if you want one job per node
unique_nodes = os.getenv("ONE_JOB_PER_NODE")

if unique_nodes is None:
    print "Running as many jobs per node as there are cores..."
    os.system("cat %s > nodefile" % pbs_nodefile)
else:
    print "Running one job per node (even if it is multi-core)..."
    os.system("uniq %s > nodefile" % pbs_nodefile)

#count the number of unique nodes
n_unique_nodes = len( os.popen("uniq %s" % pbs_nodefile, "r").readlines() )

#how many nodes have been requested?
n_nodes = len( os.popen("cat %s" % pbs_nodefile, "r").readlines() )

print "NNODES = %d : NUNIQUENODES = %d" % (n_nodes, n_unique_nodes)

#start the MPI daemons - this is necessary if PBS hasn't done it for you
os.system("mpdboot -f nodefile -n %d" % (n_unique_nodes))

#check the MPI cluster
os.system("mpdtrace")
os.system("mpdringtest")

def numberOfCompletedRuns(filename):
    if not os.path.exists(filename):
        return 0

    lines = open(filename, "r").readlines()

    max_run = 0

    for line in lines:
        m = re.search(r"--- COMPLETED RUN <(\d+)> ---", line)
        if m:
            max_run = max(max_run, int(m.groups()[0]))

    return max_run

# How many chunks have been run so far?
start_nruns = numberOfCompletedRuns(logfile)

#run the simulation
for i in range(start_nruns, nruns):
    print "Running chunk %d of %d..." % (i+1, nruns)

    # get the reference time before running a chunk
    t_start = time.time()

    chunk_cmd ="mpirun -machinefile nodefile -np %d sire_python %s >> %s 2>&1" \
                           % (n_nodes, cmd, logfile)

    print "Running command: %s" % chunk_cmd

    if (os.system(chunk_cmd) != 0):
        print "Non-zero exit status - something went wrong?"
        break

    # now get the reference time after
    t_end = time.time()

    # how long did the job take to run (in seconds)
    runtime = t_end - t_start

    print "The command took %s seconds to run." % runtime

    if runtime < 15:
        print "Simulation runtime was less than 15 seconds - something went wrong!"
        break

    if not (pbs_jobid is None):
        # how much time have we got left? (in seconds) - if it less than 10 minutes 
        # then exit immediately
        remaining_time = float( os.popen("get_remaining_queue_time", "r").readline() )

        print "We have %s seconds remaining of our queue allocation." % remaining_time

        if (remaining_time < 600):
            # Quit if we have less than ten minutes - this is because qstat has
            # at least a five minute margin of error!
            print "There is less than ten minutes left - stopping now"
            break

        elif (1.5*runtime > remaining_time):
            print "We've run out of time (%s s per iteration, but only %s s remaining)" \
                        % (runtime, remaining_time)

            break

# how many chunks have been run now?
end_nruns = numberOfCompletedRuns(logfile)

if end_nruns == start_nruns:
    #nothing was completed successfully
    print "*** ERROR *** Nothing was completed successfully during this run!"

elif end_nruns >= nruns:
    #all chunks for the simulation have finished
    print "*** All chunks of the simulation are now complete! ***"

elif pbs_jobid is None:
     print "The job hasn't finished but we can't resubmit!"

else:
     print "We ran out of time before the job finished, so resubmitting..."
     os.system("resubmit %s" % pbs_jobid)

#shut down the cluster - only necessary if PBS doesn't do this for you
os.system("mpdallexit")

#remove temporary files
if not (pbs_tmpdir is None):
    os.system("remove_pbs_tmpdir %s" % pbs_tmpdir)

#remove the nodefile
os.system("rm nodefile")
